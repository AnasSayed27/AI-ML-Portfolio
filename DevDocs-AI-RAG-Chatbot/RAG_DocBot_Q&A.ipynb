{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JcqsOyOgk7v"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers datasets rouge-score streamlit evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import evaluate # Import evaluate directly\n",
        "import numpy as np\n",
        "import streamlit as st  # For optional UI later"
      ],
      "metadata": {
        "id": "HR-2SJ14k0k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= []\n",
        "\n",
        "with open('/content/LangChain_Bot_dataset.txt', 'r') as file:\n",
        "  dataset= file.readlines()\n",
        "  print(f'Loaded {len(dataset)} lines from file.')"
      ],
      "metadata": {
        "id": "VaadiU90lDUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Embedding_model= 'all-MiniLM-L6-v2'\n",
        "embedder= SentenceTransformer(Embedding_model)\n",
        "\n",
        "Vector_DB= []\n",
        "for chunk in dataset:\n",
        "  embedding = embedder.encode([chunk])[0]\n",
        "  Vector_DB.append((chunk, embedding))\n",
        "  print(f'Add chunks to DB (shape: {embedding.shape})')"
      ],
      "metadata": {
        "id": "WY3wA_BJlcyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "  dot_product = np.dot(a, b)\n",
        "  norm_a= np.linalg.norm(a)\n",
        "  norm_b= np.linalg.norm(b)\n",
        "  return dot_product / (norm_a * norm_b)"
      ],
      "metadata": {
        "id": "bGUYy4Cop5Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_n= 3):\n",
        "  query_embedding= embedder.encode([query])[0]\n",
        "  similarities= []\n",
        "  for chunk, embedding in Vector_DB:\n",
        "    sim= cosine_similarity(query_embedding, embedding)\n",
        "    similarities.append([chunk, sim])\n",
        "\n",
        "  similarities.sort(key= lambda x: x[1], reverse= True)\n",
        "  return similarities[:top_n]"
      ],
      "metadata": {
        "id": "82TqcBMbqW1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Generation_model = 'gpt2'\n",
        "generator = pipeline('text-generation', model= Generation_model)\n",
        "\n",
        "def generate_ans(query, retrieved_knowledge):\n",
        "  context = '\\n'.join([f'- {chunk}' for chunk, _ in retrieved_knowledge])\n",
        "  prompt= f'''You are a helpful LangChain documentation assistant.\n",
        "  Use ONLY the following context to answer the technical question. Be concise, accurate, and code-focused for developers.\n",
        "  context: {context}\n",
        "  Question: {query}\n",
        "  Answer:'''\n",
        "\n",
        "  response = generator(prompt, max_new_tokens= 50, num_return_sequences= 1, do_sample= True, temperature= 0.1)[0]['generated_text']\n",
        "  answer = response.split('Answer:')[-1].strip()\n",
        "  return answer"
      ],
      "metadata": {
        "id": "sUNJAkw9rUeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_query= input('Ask any Langchain doc Question:')\n",
        "retrieved= retrieve(input_query)\n",
        "print('Retrieved Info: ')\n",
        "for chunk, sim in retrieved:\n",
        "  print(f'- (similarity {sim:.2f}) {chunk}')\n",
        "\n",
        "generated_ans= generate_ans(input_query, retrieved)\n",
        "print('\\nGenerated Answer: ', generated_ans)"
      ],
      "metadata": {
        "id": "N8iS2pJBtYfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "test_queries = [\n",
        "    (\"What is LCEL in LangChain?\", \"LCEL is LangChain Expression Language for composing chains declaratively.\"),\n",
        "    (\"How do agents work?\", \"Agents use LLMs for reasoning and tools for actions like ReAct.\"),\n",
        "    (\"Explain retrieval in LangChain.\", \"Retrieval enables RAG by embedding and fetching relevant doc chunks.\"),\n",
        "    (\"What are chains in LangChain?\", \"Chains are sequences of calls to LLMs or tools combined for a task.\"),\n",
        "    (\"What is a retriever?\", \"A retriever is an interface that returns relevant documents given a query.\"),\n",
        "    (\"How does memory work in LangChain?\", \"Memory lets chains and agents persist state across interactions.\"),\n",
        "    (\"What is RAG?\", \"RAG stands for Retrieval-Augmented Generation, combining retrieval with LLM outputs.\"),\n",
        "    (\"What are tools in LangChain?\", \"Tools are external functions or APIs that agents can call to take actions.\"),\n",
        "    (\"What is LangSmith?\", \"LangSmith is a platform for debugging, testing, and monitoring LangChain apps.\"),\n",
        "    (\"How do callbacks work?\", \"Callbacks let you log, stream, or monitor events during chain or agent execution.\")\n",
        "]\n",
        "\n",
        "scores = []\n",
        "for q, ref in test_queries:\n",
        "    ret = retrieve(q)\n",
        "    ans = generate_ans(q, ret)\n",
        "    score = rouge.compute(predictions=[ans], references=[ref])['rougeL']\n",
        "    scores.append(score)\n",
        "print(f'Avg ROUGE-L Score: {np.mean(scores):.2f}')"
      ],
      "metadata": {
        "id": "4-Q4uh7rusfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def streamlit_app():\n",
        "    st.title('RAG LangChain DocBot')\n",
        "    query = st.text_input('Ask about LangChain:')\n",
        "    if query:\n",
        "        ret = retrieve(query)\n",
        "        ans = generate_ans(query, ret)\n",
        "        st.write('Retrieved:', [c for c,_ in ret])\n",
        "        st.write('Answer:', ans)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    streamlit_app()"
      ],
      "metadata": {
        "id": "ICctvJRJoUoN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}